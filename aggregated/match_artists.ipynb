{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Check all datasets\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%load_ext autotime\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-deep')\n",
    "from ipywidgets import interact\n",
    "from ipywidgets import Controller\n",
    "\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import regex as re\n",
    "sys.path.append('/export/home/asanakoy/workspace/neural_network')\n",
    "sys.path.append('/export/home/asanakoy/workspace/art_datasets')\n",
    "sys.path.append('/export/home/asanakoy/workspace/dataset_merger')\n",
    "import make_data.dataset\n",
    "import wikiart.info.preprocess_info\n",
    "from art_utils.pandas_tools import is_null_object\n",
    "from art_utils.text_tools import extract_all_years\n",
    "import dataset_merger.read_datasets\n",
    "import dataset_merger.pymongoext as pymongoext\n",
    "from dataset_merger.match_artists import fix_sim_matrix\n",
    "from dataset_merger.match_artists import generate_matches_for_manual_check\n",
    "from dataset_merger.match_artists import get_num_top_matches\n",
    "from dataset_merger.match_artists import compute_sim_matrix, get_sim_between_rows\n",
    "from dataset_merger.match_artists import find_connected_components\n",
    "from dataset_merger.match_artists import combine_artists\n",
    "from dataset_merger.match_artists import get_merged_artists_df\n",
    "import dataset_merger.prepare_artists as prepare_artists\n",
    "from dataset_merger.prepare_artists import get_artists_with_years\n",
    "\n",
    "pd.set_option('display.max_columns', 30)\n",
    "pd.set_option('display.width', 160)\n",
    "pd.set_option('display.max_colwidth', 50)  \n",
    "pd.set_option('display.max_columns', None) \n",
    "\n",
    "# todo: merge wga, wikiart, artuk, googleart (dates checked)\n",
    "# todo: make uniform dates (bio or active if no bio available) range for every artist in the dataset\n",
    "\n",
    "# todo: check dates on others\n",
    "# todo: chach should we split naems on `&` for other datasets (beyound artuk, wikiart, googleart, wga)\n",
    "# TOD: manually check connected components (wga, artuk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artuk None\n",
      "artuk None 207585\n",
      "googleart None\n",
      "googleart None 102343\n",
      "moma None\n",
      "moma None 24708\n",
      "rijks None\n",
      "rijks None 93206\n",
      "wga None\n",
      "wga None 35373\n",
      "wiki None\n",
      "wiki None 132296\n",
      "meisterwerke None\n",
      "meisterwerke None 19647\n",
      "Total works count: 615158\n",
      "time: 24.4 s\n"
     ]
    }
   ],
   "source": [
    "dfs = dataset_merger.read_datasets.read_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.33 ms\n"
     ]
    }
   ],
   "source": [
    "# for key, df in dfs.iteritems():\n",
    "#     print key\n",
    "#     print df.columns\n",
    "#     print '==='\n",
    "    \n",
    "\n",
    "# print '--'\n",
    "# artist_names = np.unique(artist_names)\n",
    "# print 'Total unique artists num:', len(artist_names)\n",
    "# return artist_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: googleart\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 1355/4222 [00:00<00:00, 13543.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with url_wiki: 3074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4222/4222 [00:00<00:00, 14922.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total artists: 4222\n",
      "Drop artists with NaN years_range 138\n",
      "Drop 1 unknown artists\n",
      "Total artists: 4083\n",
      ":: wiki\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 1322/2384 [00:00<00:00, 13218.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Discard wrong wiki url:  http://www.myjapanesehanga.com/home/artists/asano-takeji-1900-1999\n",
      "WARNING: Discard wrong wiki url:  http://www.terminartors.com/artistprofile/Ilosvai_Varga_Istvan\n",
      "WARNING: Discard wrong wiki url:  http://www.phillipscollection.org/research/american_art/bios/knaths-bio.htm\n",
      "with url_wiki: 2081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2384/2384 [00:00<00:00, 12837.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total artists: 2384\n",
      "Drop artists with NaN years_range 6\n",
      "Drop 0 unknown artists\n",
      "Total artists: 2378\n",
      ":: wga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 1369/4573 [00:00<00:00, 13676.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with url_wiki: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4573/4573 [00:00<00:00, 12118.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total artists: 4573\n",
      "Drop artists with NaN years_range 0\n",
      "Drop 0 unknown artists\n",
      "Total artists: 4573\n",
      ":: artuk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1425/36654 [00:00<00:02, 14244.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with url_wiki: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36654/36654 [00:02<00:00, 14519.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total artists: 36654\n",
      "Drop artists with NaN years_range 3604\n",
      "Drop 0 unknown artists\n",
      "Total artists: 33050\n",
      ":: meisterwerke\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2268/2268 [00:00<00:00, 13310.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with url_wiki: 3\n",
      "Total artists: 2268\n",
      "Drop artists with NaN years_range 33\n",
      "Drop 17 unknown artists"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total artists: 2218\n",
      ":: moma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 1787/15247 [00:00<00:00, 17861.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with url_wiki: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15247/15247 [00:00<00:00, 17329.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total artists: 15247\n",
      "Drop artists with NaN years_range 3576\n",
      "Drop 1 unknown artists\n",
      "Total artists: 11670\n",
      "time: 16.2 s\n"
     ]
    }
   ],
   "source": [
    "artists_with_years_dict = dict()\n",
    "artists_with_years_dict['googleart'] = get_artists_with_years('googleart', dfs)\n",
    "artists_with_years_dict['wiki'] = get_artists_with_years('wiki', dfs)\n",
    "artists_with_years_dict['wga'] = get_artists_with_years('wga', dfs)\n",
    "artists_with_years_dict['artuk'] = get_artists_with_years('artuk', dfs)\n",
    "artists_with_years_dict['meisterwerke'] = get_artists_with_years('meisterwerke', dfs)\n",
    "artists_with_years_dict['moma'] = get_artists_with_years('moma', dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.6 ms\n"
     ]
    }
   ],
   "source": [
    "# artcyclopedia_artists = pymongoext.get_db_entries('129.206.117.36', 27017, 'artcyclopedia', 'found')\n",
    "# for x in artcyclopedia_artists:\n",
    "#     x['artcylopedia_name'] = x['name']\n",
    "#     del x['name']\n",
    "#     del x['_id']\n",
    "#     if 'content' in x:\n",
    "#         del x['content']\n",
    "# print len(artcyclopedia_artists)\n",
    "# print artcyclopedia_artists[:2]\n",
    "\n",
    "# artcyclopedia_df = pd.DataFrame.from_dict(artcyclopedia_artists)\n",
    "# artcyclopedia_df.index = artcyclopedia_df['artist_name']\n",
    "# artcyclopedia_df.rename(columns={'url': 'url_artcyclopedia'}, inplace=True)\n",
    "\n",
    "# # artists_df['url_artcyclopedia'] = artcyclopedia_df.loc[artist_names, 'url_artcyclopedia'].values\n",
    "# # print pd.notnull(artists_df['url_artcyclopedia']).sum()\n",
    "# print len(artcyclopedia_df)\n",
    "# print len(artcyclopedia_df['url_artcyclopedia'].unique())\n",
    "\n",
    "\n",
    "# artcyclopedia_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 20.7 ms\n"
     ]
    }
   ],
   "source": [
    "def get_sim_matrix(dataset_names, shape=None, force=False, n_jobs=1, num_blocks=None):\n",
    "    assert len(dataset_names) == 2\n",
    "    str_name = '-'.join(dataset_names)\n",
    "    matrix_filepath = 'sim_matrix_{}.npy'.format(str_name)\n",
    "    if os.path.exists(matrix_filepath) and not force:\n",
    "        sim = np.load(matrix_filepath)\n",
    "        if shape is not None:\n",
    "            assert shape == sim.shape, '{} != {}'.format(shape, sim.shape)\n",
    "    else:\n",
    "        sim = compute_sim_matrix(keys, artists_with_years_dict, n_jobs=n_jobs, num_blocks=num_blocks)\n",
    "        np.save(matrix_filepath, sim)\n",
    "        print 'Sim matrix saved to {}'.format(matrix_filepath)\n",
    "    return sim\n",
    "\n",
    "def get_sim_matrix_and_plot(dataset_names, force=False, n_jobs=1, num_blocks=None):\n",
    "    str_name = '-'.join(dataset_names)\n",
    "    print str_name\n",
    "    shape = tuple([len(artists_with_years_dict[dataset_names[i]]) for i in xrange(2)])\n",
    "    sim_matrix[str_name] = get_sim_matrix(dataset_names, shape=shape, force=force, \n",
    "                                          n_jobs=n_jobs, num_blocks=num_blocks)\n",
    "    print sim_matrix[str_name].shape\n",
    "    best_sim = sim_matrix[str_name].max(axis=1)\n",
    "    plt.hist(best_sim)\n",
    "    plt.show()\n",
    "\n",
    "sim_matrix = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 25.6 ms\n"
     ]
    }
   ],
   "source": [
    "def check_pairs(dataset_names, transpose=False, \n",
    "                min_sim=85, max_sim=100, \n",
    "                discard_exaclty_matched_dates=False,\n",
    "                min_k=1, min_total_works_count=0):\n",
    "    assert len(dataset_names) == 2\n",
    "    sim = sim_matrix['-'.join(dataset_names)]\n",
    "    dfs_to_merge = [artists_with_years_dict[key] for key in dataset_names]\n",
    "\n",
    "    manually_checked_matches_path = 'manually_corrected_matches/{}_manually_corrected_matches.csv'.format('-'.join(dataset_names))\n",
    "    print manually_checked_matches_path\n",
    "    if os.path.exists(manually_checked_matches_path):\n",
    "        manually_checked_matches_df = pd.read_csv(manually_checked_matches_path, index_col=0, encoding='utf-8')\n",
    "        print 'manually checked pairs:', len(manually_checked_matches_df)\n",
    "        sim = fix_sim_matrix(dataset_names, \n",
    "                             dfs_to_merge, \n",
    "                             sim, \n",
    "                             manually_checked_matches_df)\n",
    "    \n",
    "    if transpose:\n",
    "        # Transpose matches\n",
    "        dataset_names = dataset_names[::-1]\n",
    "        dfs_to_merge = dfs_to_merge[::-1]\n",
    "        sim = sim.transpose()\n",
    "\n",
    "    results_df = generate_matches_for_manual_check(dataset_names, dfs_to_merge, \n",
    "                                                   sim, min_sim=min_sim, max_sim=max_sim,\n",
    "                                                   discard_exaclty_matched_dates=discard_exaclty_matched_dates,\n",
    "                                                   min_k=min_k, \n",
    "                                                   min_total_works_count=min_total_works_count)\n",
    "    output_path = '/export/home/asanakoy/workspace/dataset_merger/aggregated/{}_check_matches.csv'.format('-'.join(dataset_names))\n",
    "    results_df.to_csv(output_path, encoding='utf-8')\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 43.2 ms\n"
     ]
    }
   ],
   "source": [
    "def correct_ids_in_manually_checked_matches(dataset_names):\n",
    "    \"\"\"\n",
    "        Check each pair in manually_checked_matches and change $artist_id to one from 'artist_ids' list (if possible)\n",
    "        if $artist_id is not currently in index of the artist_df\n",
    "    \"\"\"\n",
    "    \n",
    "    manually_checked_matches_path = 'manually_corrected_matches/{}_manually_corrected_matches.csv'.format('-'.join(dataset_names))\n",
    "    if not os.path.exists(manually_checked_matches_path):\n",
    "        raise IOError('File {} not found'.format(manually_checked_matches_path))\n",
    "    manually_checked_matches_df = pd.read_csv(manually_checked_matches_path, index_col=0, encoding='utf-8')\n",
    "    print 'manually checked pairs:', len(manually_checked_matches_df)\n",
    "\n",
    "    dfs_to_merge = [artists_with_years_dict[key] for key in dataset_names]\n",
    "    \n",
    "    def convert_to_valid_idx(artists_df, artist_id):\n",
    "        if artist_id not in artists_df.index:\n",
    "            found_idxs = np.nonzero(artists_df['artist_ids'].apply(lambda x: artist_id in x))[0]\n",
    "            if len(found_idxs):\n",
    "                assert len(found_idxs) == 1, '{}: {}'.format(artist_id, found_idxs)\n",
    "                artist_id = artists_df.index[found_idxs[0]]\n",
    "        return artist_id\n",
    "        \n",
    "    \n",
    "    def is_valid_pairs(row):\n",
    "        for i in xrange(2):\n",
    "            artist_id = row['artist_id_' + dataset_names[i]]\n",
    "            if artist_id not in dfs_to_merge[i].index:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    is_valid = manually_checked_matches_df.apply(is_valid_pairs, axis=1)\n",
    "    print 'not valid before fixes:', (~is_valid).sum()\n",
    "    print manually_checked_matches_df[~is_valid]\n",
    "    \n",
    "    for i in xrange(2):\n",
    "        manually_checked_matches_df['artist_id_' + dataset_names[i]] = \\\n",
    "            manually_checked_matches_df['artist_id_' + dataset_names[i]] \\\n",
    "            .apply(lambda x: convert_to_valid_idx(dfs_to_merge[i], x))\n",
    "    \n",
    "    is_valid = manually_checked_matches_df.apply(is_valid_pairs, axis=1)\n",
    "    print 'not valid', (~is_valid).sum()\n",
    "    print manually_checked_matches_df[~is_valid]\n",
    "    return manually_checked_matches_df\n",
    "\n",
    "# dataset_names = ['wiki+googleart', 'wga']\n",
    "# dataset_names = ['wiki+googleart+wga', 'meisterwerke']\n",
    "# manually_checked_matches_df = correct_ids_in_manually_checked_matches(dataset_names)\n",
    "# manually_checked_matches_path = 'manually_corrected_matches/{}_manually_corrected_matches.csv'.format('-'.join(dataset_names))\n",
    "# manually_checked_matches_df.to_csv(manually_checked_matches_path, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.78 ms\n"
     ]
    }
   ],
   "source": [
    "# def fix_wga_ids(matches_df):\n",
    "    \n",
    "#     def years_range_to_suffix(years_range):\n",
    "#         if not isinstance(years_range, float):\n",
    "#             assert isinstance(years_range, list)\n",
    "#             return ', ({}-{})'.format(*np.asarray(years_range, dtype=int))\n",
    "#         else:\n",
    "#             return ''\n",
    "    \n",
    "#     matches_df = matches_df.copy()\n",
    "#     fixed_bio_df = pd.read_csv('/export/home/asanakoy/workspace/wga/info/duplicate_artists_fixed_bio.csv', encoding='utf-8', header=0)\n",
    "#     for row in fixed_bio_df.itertuples():\n",
    "#         new_years_bio = map(int, re.split(r',|-', row.fixed_bio.strip()))\n",
    "#         mask = matches_df['artist_id_wga'] == row.artist_id\n",
    "#         matches_df.loc[mask, 'dates_wga'] = matches_df.loc[mask, 'dates_wga'].apply(lambda _: new_years_bio)\n",
    "#         matches_df.loc[mask, 'artist_id_wga'] = matches_df.loc[mask, 'artist_id_wga'].apply(lambda x: x.rsplit(',', 1)[0] + \n",
    "#                                                                            years_range_to_suffix(new_years_bio))\n",
    "#     return matches_df\n",
    "\n",
    "# dataset_names = ['wiki+googleart', 'wga']\n",
    "# manually_checked_matches_df = fix_wga_ids(manually_checked_matches_df)\n",
    "# manually_checked_matches_path = 'manually_corrected_matches/{}_manually_corrected_matches.csv'.format('-'.join(dataset_names))\n",
    "# manually_checked_matches_df.to_csv(manually_checked_matches_path, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 26.3 ms\n"
     ]
    }
   ],
   "source": [
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki-googleart\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/512 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num tasks: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 39/512 [00:08<03:45,  2.09it/s][Parallel(n_jobs=16)]: Done   9 tasks      | elapsed:    8.2s\n",
      " 14%|█▎        | 70/512 [00:24<05:20,  1.38it/s][Parallel(n_jobs=16)]: Done  40 tasks      | elapsed:   24.5s\n",
      " 18%|█▊        | 93/512 [00:37<04:36,  1.51it/s]"
     ]
    }
   ],
   "source": [
    "keys = ['wiki', 'googleart']\n",
    "get_sim_matrix_and_plot(keys, force=True, n_jobs=16, num_blocks=512)\n",
    "\n",
    "# keys = ['wga', 'artuk']\n",
    "# get_sim_matrix_and_plot(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "dataset_names = ['wiki', 'googleart']\n",
    "results_df = check_pairs(dataset_names, \n",
    "                transpose=False,\n",
    "                min_sim=76, max_sim=101, \n",
    "                discard_exaclty_matched_dates=True,\n",
    "                min_k=1)\n",
    "\n",
    "if len(results_df):\n",
    "    results_df['year_diff'] = results_df.apply(lambda x: [abs(x['dates_' + dataset_names[0]][i] - x['dates_' + dataset_names[1]][i]) for i in range(2)], axis=1)\n",
    "results_df\n",
    "\n",
    "# Transposed\n",
    "results_df_transposed = check_pairs(dataset_names, \n",
    "                transpose=True,\n",
    "                min_sim=76, max_sim=101, \n",
    "                discard_exaclty_matched_dates=True,\n",
    "                min_k=1)\n",
    "if len(results_df_transposed):\n",
    "    results_df_transposed['year_diff'] = results_df_transposed.apply(lambda x: [abs(x['dates_' + dataset_names[0]][i] - x['dates_' + dataset_names[1]][i]) for i in range(2)], axis=1)\n",
    "\n",
    "if len(results_df):\n",
    "    appended = results_df.append(results_df_transposed, ignore_index=True)\n",
    "    appended = appended.drop_duplicates(['artist_id_' + key for key in dataset_names])\n",
    "    appended = appended[results_df.columns]\n",
    "else:\n",
    "    appended = results_df_transposed\n",
    "\n",
    "# appended = results_df\n",
    "output_path = '/export/home/asanakoy/workspace/dataset_merger/aggregated/{}_check_matches.csv'.format('-'.join(dataset_names))\n",
    "appended.to_csv(output_path, encoding='utf-8')\n",
    "print 'Total rows', len(appended)\n",
    "appended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sim_values(dataset_names, artist_ids):\n",
    "    cur_dfs = [artists_with_years_dict[key] for key in dataset_names]\n",
    "    indices = [0, 0]\n",
    "    for i in xrange(2):\n",
    "        idx = np.nonzero(cur_dfs[i]['artist_id'] == artist_ids[i])[0][0]\n",
    "        print cur_dfs[i].iloc[idx]\n",
    "        print '---'\n",
    "        indices[i] = idx\n",
    "    print indices\n",
    "    sim = get_sim_matrix(dataset_names, force=False)\n",
    "    print 'sim=', sim[indices[0], indices[1]]\n",
    "\n",
    "check_sim_values(['wiki', 'googleart'], ['wiki_johan-hendrik-weissenbruch', 'googleart_m026t8y5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = ['wiki', 'googleart']\n",
    "sim = sim_matrix['-'.join(dataset_names)]\n",
    "dfs_to_merge = [artists_with_years_dict[key] for key in dataset_names]\n",
    "\n",
    "merged_df = get_merged_artists_df(dataset_names, dfs_to_merge, sim, allow_big_components=False)\n",
    "artists_with_years_dict['wiki+googleart'] = merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "keys = ['wiki+googleart', 'wga']\n",
    "get_sim_matrix_and_plot(keys, force=True, n_jobs=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "dataset_names = ['wiki+googleart', 'wga']\n",
    "results_df = check_pairs(dataset_names, \n",
    "                transpose=False,\n",
    "                min_sim=80, max_sim=121, \n",
    "                discard_exaclty_matched_dates=False,\n",
    "                min_k=2)\n",
    "\n",
    "if len(results_df):\n",
    "    results_df['year_diff'] = results_df.apply(lambda x: [abs(x['dates_' + dataset_names[0]][i] - x['dates_' + dataset_names[1]][i]) for i in range(2)], axis=1)\n",
    "results_df\n",
    "\n",
    "# Transposed\n",
    "results_df_transposed = check_pairs(dataset_names, \n",
    "                transpose=True,\n",
    "                min_sim=80, max_sim=121, \n",
    "                discard_exaclty_matched_dates=False,\n",
    "                min_k=2)\n",
    "if len(results_df_transposed):\n",
    "    results_df_transposed['year_diff'] = results_df_transposed.apply(lambda x: [abs(x['dates_' + dataset_names[0]][i] - x['dates_' + dataset_names[1]][i]) for i in range(2)], axis=1)\n",
    "\n",
    "if len(results_df):\n",
    "    appended = results_df.append(results_df_transposed, ignore_index=True)\n",
    "    appended = appended.drop_duplicates(['artist_id_wga', 'artist_id_wiki+googleart'])\n",
    "    appended = appended[results_df.columns]\n",
    "else:\n",
    "    appended = results_df_transposed\n",
    "\n",
    "# appended = results_df\n",
    "output_path = '/export/home/asanakoy/workspace/dataset_merger/aggregated/{}_check_matches.csv'.format('-'.join(dataset_names))\n",
    "appended.to_csv(output_path, encoding='utf-8')\n",
    "print 'Total rows', len(appended)\n",
    "appended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# artists_with_years_dict['googleart']['url_wiki'] = artists_with_years_dict['googleart']['url_wiki'].apply(lambda x: 'https://en.wikipedia.org/wiki/ernest_meissonier' if x == 'http://en.wikipedia.org/wiki/jean-louis-ernest_meissonier' else x)\n",
    "\n",
    "\n",
    "# ids = list()\n",
    "# for comp in big_comp_wiki_googleart:\n",
    "#     comp = sorted(list(comp))http://localhost:8893/notebooks/dataset_merger/aggregated/match_artists.ipynb#\n",
    "#     ids.extend(comp[:2])\n",
    "# artists_with_years_dict['wiki+googleart'][['artist_name', 'years_range', 'url_wiki']].iloc[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_names = ['wiki+googleart', 'wga']\n",
    "sim = sim_matrix['-'.join(dataset_names)]\n",
    "dfs_to_merge = [artists_with_years_dict[key] for key in dataset_names]\n",
    "\n",
    "merged_df = get_merged_artists_df(dataset_names, dfs_to_merge, sim, allow_big_components=False)\n",
    "artists_with_years_dict['+'.join(dataset_names)] = merged_df\n",
    "merged_df.to_hdf('artists_{}.hdf5'.format('+'.join(dataset_names)), 'df', mode='w')\n",
    "# merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists_with_years_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = ['wiki', 'googleart', 'wga', 'wiki+googleart+wga']\n",
    "for key in dataset_names:\n",
    "    print '{}: {} unique artists'.format(key, len(artists_with_years_dict[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### MEISTERWERKE #######\n",
    "\n",
    "%autoreload\n",
    "keys = ['wiki+googleart+wga', 'meisterwerke']\n",
    "get_sim_matrix_and_plot(keys, force=True, n_jobs=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs_from_the_same_dataset(dataset_names, df, min_sim=102):\n",
    "    df = df[df['score'] >= min_sim].copy()\n",
    "    fir_ids_col = 'artist_id_' + dataset_names[0]\n",
    "    sec_ids_col = 'artist_id_' + dataset_names[1]\n",
    "    df.index = df[sec_ids_col]\n",
    "    df_gr = pd.DataFrame(data={sec_ids_col: \n",
    "                            df.groupby(fir_ids_col)[sec_ids_col].apply(list)})\n",
    "    df_gr['dates'] = df_gr[sec_ids_col].apply(lambda x: [df.at[el, 'dates_' + dataset_names[1]] for el in x])\n",
    "    df_gr[sec_ids_col] = df_gr[sec_ids_col].apply(lambda x: ';'.join(x))\n",
    "    return df_gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##### MEISTERWERKE #######\n",
    "%autoreload\n",
    "dataset_names = ['wiki+googleart+wga', 'meisterwerke']\n",
    "results_df = []\n",
    "results_df = check_pairs(dataset_names, \n",
    "                transpose=False,\n",
    "                min_sim=85, max_sim=101, \n",
    "                discard_exaclty_matched_dates=True,\n",
    "                min_k=1,\n",
    "                min_total_works_count=1)\n",
    "\n",
    "if len(results_df):\n",
    "    results_df['year_diff'] = results_df.apply(lambda x: [abs(x['dates_' + dataset_names[0]][i] - x['dates_' + dataset_names[1]][i]) for i in range(2)], axis=1)\n",
    "results_df\n",
    "\n",
    "# Transposed\n",
    "results_df_transposed = []\n",
    "results_df_transposed = check_pairs(dataset_names, \n",
    "                transpose=True,\n",
    "                min_sim=85, max_sim=101, \n",
    "                discard_exaclty_matched_dates=True,\n",
    "                min_k=1,\n",
    "                min_total_works_count=1)\n",
    "if len(results_df_transposed):\n",
    "    results_df_transposed['year_diff'] = results_df_transposed.apply(lambda x: [abs(x['dates_' + dataset_names[0]][i] - x['dates_' + dataset_names[1]][i]) for i in range(2)], axis=1)\n",
    "\n",
    "if len(results_df) and len(results_df_transposed):\n",
    "    appended = results_df.append(results_df_transposed, ignore_index=True)\n",
    "    appended = appended.drop_duplicates(['artist_id_' + key for key in dataset_names])\n",
    "    appended = appended[results_df.columns]\n",
    "#     assert len(appended['artist_id_meisterwerke'].unique()) == len(appended), len(appended['artist_id_meisterwerke'].unique())\n",
    "    appended.sort_values(by='artist_id_wiki+googleart+wga', inplace=True)\n",
    "elif len(results_df):\n",
    "    appended = results_df\n",
    "elif len(results_df_transposed):\n",
    "    appended = results_df_transposed\n",
    "else:\n",
    "    appended = pd.DataFrame()\n",
    "    \n",
    "# appended.sort_values(by='artist_id_wiki+googleart+wga', inplace=True)\n",
    "# appended.sort_values(by='artist_id_meisterwerke', inplace=True)\n",
    "\n",
    "\n",
    "from os.path import expanduser\n",
    "output_path = expanduser('~/workspace/dataset_merger/aggregated/{}_check_matches.csv'.format('-'.join(dataset_names)))\n",
    "appended.to_csv(output_path, encoding='utf-8')\n",
    "print 'Total rows', len(appended)\n",
    "print dataset_names\n",
    "appended\n",
    "# duplicates_df = get_pairs_from_the_same_dataset(dataset_names[::-1], appended, min_sim=100)\n",
    "# output_path = expanduser('~/workspace/dataset_merger/aggregated/{}_duplicates.csv'.format(dataset_names[1]))\n",
    "# duplicates_df.to_csv(output_path, encoding='utf-8')\n",
    "# duplicates_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# results_df = appended1\n",
    "# appended = results_df.append(results_df_transposed, ignore_index=True)\n",
    "# appended = appended.drop_duplicates(['artist_id_' + key for key in dataset_names])\n",
    "# appended = appended[results_df.columns]\n",
    "# #     assert len(appended['artist_id_meisterwerke'].unique()) == len(appended), len(appended['artist_id_meisterwerke'].unique())\n",
    "# appended.sort_values(by='artist_id_wiki+googleart+wga', inplace=True)\n",
    "# appended.to_csv(output_path, encoding='utf-8')\n",
    "# appended\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### MEISTERWERKE #######\n",
    "dataset_names = ['wiki+googleart+wga', 'meisterwerke']\n",
    "sim = sim_matrix['-'.join(dataset_names)]\n",
    "dfs_to_merge = [artists_with_years_dict[key] for key in dataset_names]\n",
    "\n",
    "# artists_with_years_dict['+'.join(dataset_names)] = get_merged_artists_df(dataset_names, dfs_to_merge, sim)\n",
    "\n",
    "\n",
    "assert len(dataset_names) == len(dfs_to_merge)\n",
    "assert sim.shape == (len(dfs_to_merge[0]), len(dfs_to_merge[1])), sim.shape\n",
    "\n",
    "\n",
    "manually_checked_matches_path = 'manually_corrected_matches/{}_manually_corrected_matches.csv'.format(\n",
    "    '-'.join(dataset_names))\n",
    "if os.path.exists(manually_checked_matches_path):\n",
    "    manually_checked_matches_df = pd.read_csv(manually_checked_matches_path, index_col=0,\n",
    "                                              encoding='utf-8')\n",
    "    sim = fix_sim_matrix(dataset_names,\n",
    "                                dfs_to_merge,\n",
    "                                sim,\n",
    "                                manually_checked_matches_df)\n",
    "\n",
    "connected_components = find_connected_components(sim, min_sim=100)\n",
    "big_comp = [x for x in connected_components if len(x) > 2]\n",
    "small_comp = [x for x in connected_components if len(x) <= 2]\n",
    "# connected_components = small_comp + [{el} for x in big_comp for el in x]\n",
    "# big_comp = [x for x in connected_components if len(x) > 2]\n",
    "\n",
    "print 'num big_comp', len(big_comp)\n",
    "# assert len(big_comp) == 0, len(big_comp)\n",
    "big_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: filter out big components. Recheck them\n",
    "# TODO: merge images wga wiki. googleart\n",
    "# these are mostly followers of the artists in the meisterwerke and workshops.\n",
    "# But they have to be manually checked\n",
    "big_comp\n",
    "items_list = []\n",
    "for comp in big_comp:\n",
    "    for item_id in comp:\n",
    "        df_idx = item_id / int(1e6)\n",
    "#         if df_idx == 0:\n",
    "#             continue\n",
    "        artist_idx = item_id % int(1e6)\n",
    "        items_list.append(dfs_to_merge[df_idx].iloc[artist_idx][['artist_id', 'artist_names', 'years_range']])\n",
    "pd.DataFrame(items_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "dataset_names = ['wiki+googleart+wga', 'meisterwerke']\n",
    "sim = sim_matrix['-'.join(dataset_names)]\n",
    "dfs_to_merge = [artists_with_years_dict[key] for key in dataset_names]\n",
    "\n",
    "merged_df = get_merged_artists_df(dataset_names, dfs_to_merge, sim, \n",
    "                                  split_big_components=False, \n",
    "                                  allow_big_components=False)\n",
    "artists_with_years_dict['+'.join(dataset_names)] = merged_df\n",
    "print 'Num unique artists:', len(merged_df)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = ['wiki', 'googleart', 'wga', 'meisterwerke', 'moma', 'wiki+googleart+wga+meisterwerke']\n",
    "for key in dataset_names:\n",
    "    print '{}: {} unique artists'.format(key, len(artists_with_years_dict[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### MOMA #######\n",
    "\n",
    "%autoreload\n",
    "keys = ['wiki+googleart+wga+meisterwerke', 'moma']\n",
    "mask = (artists_with_years_dict['moma']['wikidata_qid'].notnull() | \n",
    "        artists_with_years_dict['moma']['years_bio'].notnull() |\n",
    "        artists_with_years_dict['moma']['works_count'] > 0)\n",
    "artists_with_years_dict['moma'] = artists_with_years_dict['moma'][mask].copy()\n",
    "get_sim_matrix_and_plot(keys, force=True, n_jobs=40, num_blocks=160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### MOMA #######\n",
    "%autoreload\n",
    "dataset_names = ['wiki+googleart+wga+meisterwerke', 'moma']\n",
    "results_df = []\n",
    "results_df = check_pairs(dataset_names, \n",
    "                transpose=False,\n",
    "                min_sim=84, max_sim=101, \n",
    "                discard_exaclty_matched_dates=True,\n",
    "                min_k=1,\n",
    "                min_total_works_count=0)\n",
    "\n",
    "if len(results_df):\n",
    "    results_df['year_diff'] = results_df.apply(lambda x: [abs(x['dates_' + dataset_names[0]][i] - x['dates_' + dataset_names[1]][i]) for i in range(2)], axis=1)\n",
    "results_df\n",
    "\n",
    "# Transposed\n",
    "results_df_transposed = []\n",
    "results_df_transposed = check_pairs(dataset_names, \n",
    "                transpose=True,\n",
    "                min_sim=84, max_sim=101, \n",
    "                discard_exaclty_matched_dates=True,\n",
    "                min_k=1,\n",
    "                min_total_works_count=0)\n",
    "if len(results_df_transposed):\n",
    "    results_df_transposed['year_diff'] = results_df_transposed.apply(lambda x: [abs(x['dates_' + dataset_names[0]][i] - x['dates_' + dataset_names[1]][i]) for i in range(2)], axis=1)\n",
    "\n",
    "if len(results_df) and len(results_df_transposed):\n",
    "    appended = results_df.append(results_df_transposed, ignore_index=True)\n",
    "    appended = appended.drop_duplicates(['artist_id_' + key for key in dataset_names])\n",
    "    appended = appended[results_df.columns]\n",
    "#     assert len(appended['artist_id_meisterwerke'].unique()) == len(appended), len(appended['artist_id_meisterwerke'].unique())\n",
    "    appended.sort_values(by='artist_id_' + dataset_names[0], inplace=True)\n",
    "elif len(results_df):\n",
    "    appended = results_df\n",
    "elif len(results_df_transposed):\n",
    "    appended = results_df_transposed\n",
    "else:\n",
    "    appended = pd.DataFrame()\n",
    "    \n",
    "# appended.sort_values(by='artist_id_wiki+googleart+wga', inplace=True)\n",
    "# appended.sort_values(by='artist_id_meisterwerke', inplace=True)\n",
    "\n",
    "\n",
    "from os.path import expanduser\n",
    "output_path = expanduser('~/workspace/dataset_merger/aggregated/{}_check_matches.csv'.format('-'.join(dataset_names)))\n",
    "appended.to_csv(output_path, encoding='utf-8')\n",
    "print 'Total rows', len(appended)\n",
    "print dataset_names\n",
    "appended\n",
    "# duplicates_df = get_pairs_from_the_same_dataset(dataset_names[::-1], appended, min_sim=100)\n",
    "# output_path = expanduser('~/workspace/dataset_merger/aggregated/{}_duplicates.csv'.format(dataset_names[1]))\n",
    "# duplicates_df.to_csv(output_path, encoding='utf-8')\n",
    "# duplicates_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "dataset_names = ['wiki+googleart+wga+meisterwerke', 'moma']\n",
    "sim = sim_matrix['-'.join(dataset_names)]\n",
    "dfs_to_merge = [artists_with_years_dict[key] for key in dataset_names]\n",
    "\n",
    "merged_df = get_merged_artists_df(dataset_names, dfs_to_merge, sim, \n",
    "                                  split_big_components=False, \n",
    "                                  allow_big_components=False) # TODO:\n",
    "merged_df = merged_df[merged_df['works_count'] > 0]\n",
    "artists_with_years_dict['+'.join(dataset_names)] = merged_df\n",
    "print 'Num unique artists:', len(merged_df)\n",
    "# merged_df.to_hdf('info/artists_{}_v0.93.hdf5'.format('+'.join(dataset_names)), 'df', mode='w')\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in artists_with_years_dict.keys():\n",
    "    print '{}: {} unique artists'.format(key, len(artists_with_years_dict[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### ARTUK #######\n",
    "%autoreload\n",
    "keys = ['wiki+googleart+wga+meisterwerke+moma', 'artuk']\n",
    "# artists_with_years_dict['artuk'] = get_artists_with_years('artuk', dfs)\n",
    "mask = (artists_with_years_dict['artuk']['years_bio'].notnull() |\n",
    "        artists_with_years_dict['artuk']['wikidata_qid'].notnull() | \n",
    "        artists_with_years_dict['artuk']['artist_id_degruyter'].notnull() | \n",
    "        (artists_with_years_dict['artuk']['works_count'] > 6))\n",
    "artists_with_years_dict['artuk'] = artists_with_years_dict['artuk'].loc[mask].copy()\n",
    "print 'Num artists now:', len(artists_with_years_dict['artuk'])\n",
    "get_sim_matrix_and_plot(keys, force=True, n_jobs=40, num_blocks=160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##### ARTUK #######\n",
    "%autoreload\n",
    "dataset_names = ['wiki+googleart+wga+meisterwerke+moma', 'artuk']\n",
    "results_df = []\n",
    "results_df = check_pairs(dataset_names, \n",
    "                transpose=False,\n",
    "                min_sim=87, max_sim=121, \n",
    "                discard_exaclty_matched_dates=False,\n",
    "                min_k=2,\n",
    "                min_total_works_count=0)\n",
    "\n",
    "if len(results_df):\n",
    "    results_df['year_diff'] = results_df.apply(lambda x: [abs(x['dates_' + dataset_names[0]][i] - x['dates_' + dataset_names[1]][i]) for i in range(2)], axis=1)\n",
    "results_df\n",
    "\n",
    "# Transposed\n",
    "results_df_transposed = []\n",
    "results_df_transposed = check_pairs(dataset_names, \n",
    "                transpose=True,\n",
    "                min_sim=87, max_sim=121, \n",
    "                discard_exaclty_matched_dates=False,\n",
    "                min_k=2,\n",
    "                min_total_works_count=0)\n",
    "if len(results_df_transposed):\n",
    "    results_df_transposed['year_diff'] = results_df_transposed.apply(lambda x: [abs(x['dates_' + dataset_names[0]][i] - x['dates_' + dataset_names[1]][i]) for i in range(2)], axis=1)\n",
    "\n",
    "if len(results_df) and len(results_df_transposed):\n",
    "    appended = results_df.append(results_df_transposed, ignore_index=True)\n",
    "    appended = appended.drop_duplicates(['artist_id_' + key for key in dataset_names])\n",
    "    appended = appended[results_df.columns]\n",
    "#     assert len(appended['artist_id_meisterwerke'].unique()) == len(appended), len(appended['artist_id_meisterwerke'].unique())\n",
    "    appended.sort_values(by='artist_id_' + dataset_names[0], inplace=True)\n",
    "elif len(results_df):\n",
    "    appended = results_df\n",
    "elif len(results_df_transposed):\n",
    "    appended = results_df_transposed\n",
    "else:\n",
    "    appended = pd.DataFrame()\n",
    "    \n",
    "# appended.sort_values(by='artist_id_wiki+googleart+wga', inplace=True)\n",
    "# appended.sort_values(by='artist_id_meisterwerke', inplace=True)\n",
    "\n",
    "\n",
    "from os.path import expanduser\n",
    "output_path = expanduser('~/workspace/dataset_merger/aggregated/{}_check_matches.csv'.format('-'.join(dataset_names)))\n",
    "appended.to_csv(output_path, encoding='utf-8')\n",
    "print 'Total rows', len(appended)\n",
    "print dataset_names\n",
    "appended\n",
    "# duplicates_df = get_pairs_from_the_same_dataset(dataset_names[::-1], appended, min_sim=100)\n",
    "# output_path = expanduser('~/workspace/dataset_merger/aggregated/{}_duplicates.csv'.format(dataset_names[1]))\n",
    "# duplicates_df.to_csv(output_path, encoding='utf-8')\n",
    "# duplicates_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "dataset_names = ['wiki+googleart+wga+meisterwerke+moma', 'artuk']\n",
    "sim = sim_matrix['-'.join(dataset_names)]\n",
    "dfs_to_merge = [artists_with_years_dict[key] for key in dataset_names]\n",
    "\n",
    "merged_df = get_merged_artists_df(dataset_names, dfs_to_merge, sim, \n",
    "                                  split_big_components=False, \n",
    "                                  allow_big_components=True)\n",
    "merged_df = merged_df[merged_df['works_count'] > 0]\n",
    "artists_with_years_dict['+'.join(dataset_names)] = merged_df\n",
    "print 'Num unique artists:', len(merged_df)\n",
    "# merged_df.to_hdf('info/artists_{}_v0.93.hdf5'.format('+'.join(dataset_names)), 'df', mode='w')\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### RIJKS #######\n",
    "%autoreload\n",
    "keys = ['wiki+googleart+wga+meisterwerke+moma+artuk', 'rijks']\n",
    "dfs['rijks'] = dataset_merger.read_datasets.read_datasets(names=['rijks'])['rijks']\n",
    "artists_with_years_dict['rijks'] = get_artists_with_years('rijks', dfs)\n",
    "# mask = (artists_with_years_dict['rijks']['wikidata_qid'].notnull() | \n",
    "#         artists_with_years_dict['rijks']['artist_id_degruyter'].notnull() | \n",
    "#        (artists_with_years_dict['rijks']['works_count'] > 10))\n",
    "# artists_with_years_dict['rijks'] = artists_with_years_dict['rijks'].loc[mask].copy()\n",
    "print 'Num artists now:', len(artists_with_years_dict['rijks'])\n",
    "get_sim_matrix_and_plot(keys, force=True, n_jobs=40, num_blocks=160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##### RIJKS #######\n",
    "%autoreload\n",
    "dataset_names = ['wiki+googleart+wga+meisterwerke+moma+artuk', 'rijks']\n",
    "results_df = []\n",
    "results_df = check_pairs(dataset_names, \n",
    "                transpose=False,\n",
    "                min_sim=89, max_sim=121, \n",
    "                discard_exaclty_matched_dates=False,\n",
    "                min_k=2,\n",
    "                min_total_works_count=0)\n",
    "\n",
    "if len(results_df):\n",
    "    results_df['year_diff'] = results_df.apply(lambda x: [abs(x['dates_' + dataset_names[0]][i] - x['dates_' + dataset_names[1]][i]) for i in range(2)], axis=1)\n",
    "results_df\n",
    "\n",
    "# Transposed\n",
    "results_df_transposed = []\n",
    "results_df_transposed = check_pairs(dataset_names, \n",
    "                transpose=True,\n",
    "                min_sim=89, max_sim=121, \n",
    "                discard_exaclty_matched_dates=False,\n",
    "                min_k=2,\n",
    "                min_total_works_count=0)\n",
    "if len(results_df_transposed):\n",
    "    results_df_transposed['year_diff'] = results_df_transposed.apply(lambda x: [abs(x['dates_' + dataset_names[0]][i] - x['dates_' + dataset_names[1]][i]) for i in range(2)], axis=1)\n",
    "\n",
    "if len(results_df) and len(results_df_transposed):\n",
    "    appended = results_df.append(results_df_transposed, ignore_index=True)\n",
    "    appended = appended.drop_duplicates(['artist_id_' + key for key in dataset_names])\n",
    "    appended = appended[results_df.columns]\n",
    "#     assert len(appended['artist_id_meisterwerke'].unique()) == len(appended), len(appended['artist_id_meisterwerke'].unique())\n",
    "    appended.sort_values(by='artist_id_' + dataset_names[0], inplace=True)\n",
    "elif len(results_df):\n",
    "    appended = results_df\n",
    "elif len(results_df_transposed):\n",
    "    appended = results_df_transposed\n",
    "else:\n",
    "    appended = pd.DataFrame()\n",
    "    \n",
    "# appended.sort_values(by='artist_id_wiki+googleart+wga', inplace=True)\n",
    "# appended.sort_values(by='artist_id_meisterwerke', inplace=True)\n",
    "\n",
    "\n",
    "from os.path import expanduser\n",
    "output_path = expanduser('~/workspace/dataset_merger/aggregated/{}_check_matches.csv'.format('-'.join(dataset_names)))\n",
    "appended.to_csv(output_path, encoding='utf-8')\n",
    "print 'Total rows', len(appended)\n",
    "print dataset_names\n",
    "appended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "dataset_names = ['wiki+googleart+wga+meisterwerke+moma+artuk', 'rijks']\n",
    "sim = sim_matrix['-'.join(dataset_names)]\n",
    "dfs_to_merge = [artists_with_years_dict[key] for key in dataset_names]\n",
    "\n",
    "merged_df = get_merged_artists_df(dataset_names, dfs_to_merge, sim, \n",
    "                                  split_big_components=False, \n",
    "                                  allow_big_components=True)\n",
    "merged_df = merged_df[merged_df['works_count'] > 0]\n",
    "artists_with_years_dict['+'.join(dataset_names)] = merged_df\n",
    "print 'Num unique artists:', len(merged_df)\n",
    "merged_df.to_hdf('info/artists_{}_v1.00.hdf5'.format('+'.join(dataset_names)), 'df', mode='w')\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from fuzzywuzzy import fuzz\n",
    "# def get_years_range_sim(range_a, range_b, is_bio=False, max_dist=85.0, max_delta=10,\n",
    "#                         allow_include=True,\n",
    "#                         allow_second_range_with_nan=False, bio_range=None):\n",
    "#     \"\"\"\n",
    "\n",
    "#     Args:\n",
    "#         range_a:\n",
    "#         range_b:\n",
    "#         is_bio:\n",
    "\n",
    "#     Returns: similarity score from 0 to 1.0\n",
    "\n",
    "#     \"\"\"\n",
    "#     if (np.isnan(range_a[0]) or np.isnan(range_b[0])) and \\\n",
    "#             (np.isnan(range_a[1]) or np.isnan(range_b[1])):\n",
    "#         raise ValueError('Cannot compare 2 ranges with nans: {}, {}'.format(range_a, range_b))\n",
    "\n",
    "#     if allow_second_range_with_nan:\n",
    "#         range_a, range_b = list(range_a), list(range_b)\n",
    "#         if np.isnan(range_b[0]):\n",
    "#             assert not np.isnan(range_b[1]), 'one of the dates must be not none!'\n",
    "#             range_b[0] = range_a[0]\n",
    "#             if range_b[0] > range_b[1] + 3:\n",
    "#                 return 0\n",
    "#             max_dist = 0\n",
    "#         elif np.isnan(range_b[1]):\n",
    "#             assert not np.isnan(range_b[0]), 'one of the dates must be not none!'\n",
    "#             range_b[1] = range_a[1]\n",
    "#             if range_b[1] < range_b[0] - 3:\n",
    "#                 return 0\n",
    "#             max_dist = 0\n",
    "\n",
    "#     assert isinstance(range_a, list) and isinstance(range_b, list)\n",
    "#     ranges = [range_a, range_b]\n",
    "#     ranges.sort()\n",
    "#     for i in xrange(2):\n",
    "#         ranges[i] = np.asarray(ranges[i], dtype=float)\n",
    "#     assert np.all(np.isfinite(ranges)), ranges\n",
    "\n",
    "#     if bio_range is not None:\n",
    "#         max_dist = 0  # disable if we have bio\n",
    "#         if allow_include and not np.isnan(bio_range[0]) and (ranges[0][0] < bio_range[0] - 1):\n",
    "#             allow_include = False\n",
    "#         if allow_include and not np.isnan(bio_range[1]) and (bio_range[1] + 1 < max(ranges[0][1], ranges[1][1])):\n",
    "#             allow_include = False\n",
    "\n",
    "#     if allow_include and ((ranges[0][0] <= ranges[1][0] and ranges[1][1] <= ranges[0][1]) or\n",
    "#                           (ranges[1][0] <= ranges[0][0] and ranges[0][1] <= ranges[1][1])):\n",
    "#         # one is subset of another\n",
    "#         sim = 1.0\n",
    "#     else:\n",
    "#         dist = abs(ranges[1][1] - ranges[0][0])\n",
    "#         delta = max(abs(ranges[1][1] - ranges[0][1]), abs(ranges[1][0] - ranges[0][0]))\n",
    "#         assert dist >= 0, ranges\n",
    "#         assert delta >= 0, ranges\n",
    "#         # TODO: process if artist is still alive (death > 2017 ~2099)\n",
    "#         # TODO: if one of the ranges is bio, allow only delta < 10\n",
    "#         if dist <= max_dist or delta <= max_delta:\n",
    "#             sim = 1.0\n",
    "#         else:\n",
    "#             if delta <= max_delta + 10:\n",
    "#                 sim = 1 - (min((delta - max_delta), 10) / 10.0) ** 2\n",
    "#             else:\n",
    "#                 sim = 1 - np.sqrt(min((dist - max_dist), 35) / 35.0)\n",
    "#     return sim\n",
    "\n",
    "\n",
    "# def get_names_sim(names_a, names_b):\n",
    "#     \"\"\"\n",
    "\n",
    "#     Args:\n",
    "#         names_a:\n",
    "#         names_b:\n",
    "\n",
    "#     Returns: similarity score from 0 to 100\n",
    "\n",
    "#     \"\"\"\n",
    "#     scores = list()\n",
    "#     for a in names_a:\n",
    "#         for b in names_b:\n",
    "#             scores.append(fuzz.token_set_ratio(a, b, force_ascii=False))\n",
    "#     return max(scores)\n",
    "\n",
    "\n",
    "# def get_sim_urls_wiki(url_a, url_b):\n",
    "#     urls = [url_a, url_b]\n",
    "#     for i in xrange(len(urls)):\n",
    "#         url = urllib.unquote(urls[i].encode('utf8')).decode('utf-8')\n",
    "#         url = url.strip().lower()\n",
    "#         tokens = url.split('wikipedia.org')\n",
    "#         if len(tokens) != 2:\n",
    "#             raise ValueError(u'Not a wiki link:{}'.format(url))\n",
    "#         urls[i] = tokens[1]\n",
    "#     return float(urls[0] == urls[1]) * 101\n",
    "\n",
    "\n",
    "# def get_sim_between_rows(row_a, row_b):\n",
    "#     names_a = row_a.artist_names\n",
    "#     years_range_a = row_a.years_range\n",
    "#     if hasattr(row_a, 'wikidata_qid') and hasattr(row_b, 'wikidata_qid') and \\\n",
    "#             not is_null_object(row_a.wikidata_qid) and not is_null_object(row_b.wikidata_qid):\n",
    "#         cur_sim = (row_a.wikidata_qid.strip().lower() == row_b.wikidata_qid.strip().lower()) * 101\n",
    "#     elif hasattr(row_a, 'url_wiki') and hasattr(row_b, 'url_wiki') and \\\n",
    "#          not is_null_object(row_a.url_wiki) and not is_null_object(row_b.url_wiki) and \\\n",
    "#          'wikipedia.org' in row_a.url_wiki and \\\n",
    "#          'wikipedia.org' in row_b.url_wiki:\n",
    "#         cur_sim = get_sim_urls_wiki(row_a.url_wiki, row_b.url_wiki)\n",
    "#     elif hasattr(row_a, 'artist_id_degruyter') and hasattr(row_b, 'artist_id_degruyter') and \\\n",
    "#          not is_null_object(row_a.artist_id_degruyter) and not is_null_object(row_b.artist_id_degruyter):\n",
    "#         cur_sim = (row_a.artist_id_degruyter == row_b.artist_id_degruyter) * 101\n",
    "#     else:\n",
    "#         print 'General sim'\n",
    "#         names_b = row_b.artist_names\n",
    "#         years_range_b = row_b.years_range\n",
    "#         years_sim = get_years_range_sim(years_range_a, years_range_b, max_dist=-1)\n",
    "#         if years_sim < 0.5:\n",
    "#             cur_sim = years_sim * 0.9\n",
    "#         else:\n",
    "#             names_sim = get_names_sim(names_a, names_b)\n",
    "#             print 'names_sim', names_sim, 'years_sim', years_sim\n",
    "#             cur_sim = years_sim * names_sim\n",
    "#     return cur_sim\n",
    "\n",
    "\n",
    "# for item in appended.itertuples():\n",
    "#     idx = item.idx[::-1]\n",
    "# #     print map(len, [artists_with_years_dict[x] for x in dataset_names])\n",
    "#     row_a = list(artists_with_years_dict[dataset_names[0]].iloc[[idx[0]]].itertuples())[0]\n",
    "#     row_b = list(artists_with_years_dict[dataset_names[1]].iloc[[idx[1]]].itertuples())[0]\n",
    "    \n",
    "#     new_sim = get_sim_between_rows(row_a, row_b)\n",
    "#     print '{}->{}'.format(sim_matrix['wiki+googleart+wga+meisterwerke+moma+artuk-rijks'][idx[0], idx[1]], \n",
    "#                           new_sim)\n",
    "# #     sim_matrix['wiki+googleart+wga+meisterwerke+moma+artuk-rijks'][idx[0], idx[1]] = new_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z[z.index.str.startswith('artuk_austin, samuel,')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z[z.index.str.startswith('artuk_colkett, samuel david,')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = artists_with_years_dict['rijks']\n",
    "z[z.index.str.startswith('rijks_rjgd-suzuki-kiitsu')].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = artists_with_years_dict['wiki+googleart+wga+meisterwerke+moma+artuk']\n",
    "z[z.index.str.startswith('artuk_denning, stephen poyntz, (1795-1864)')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
